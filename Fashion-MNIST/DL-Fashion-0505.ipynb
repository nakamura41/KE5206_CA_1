{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.engine.training import Model\n",
    "from keras.layers import Activation\n",
    "from keras.models import load_model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import fashion_mnist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi GPU Training Test (add by ryan: the important one is the multi_gpu_model import from Keras)\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset\n",
    "K.clear_session()\n",
    "\n",
    "# This one is to test whether Keras can see the GPUs\n",
    "K.get_session().list_devices() \n",
    "\n",
    "# Add by Ryan: configure tensorflow backend to increase the GPU memory allocation gradually and stopping the memory allocation failure during traiing\n",
    "config = tf.ConfigProto( device_count = {'GPU':2, 'CPU':4})\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assist function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(arr):\n",
    "  labels = np.zeros( (arr.shape[0],1) )\n",
    "  for i in range(0,arr.shape[0]):\n",
    "    row = arr[i,]\n",
    "    labels[i,0]=np.where(row==np.max(row,axis=0))[0]\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model_details):\n",
    "\n",
    "    # Create sub-plots\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    # Summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_details.history['loss'])+1),model_details.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_details.history['val_loss'])+1),model_details.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_details.history['loss'])+1),len(model_details.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # Summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_details.history['acc'])+1),model_details.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_details.history['val_acc'])+1),model_details.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_details.history['acc'])+1),len(model_details.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_resize(org_imgs, target_size):\n",
    "  x_imgs = []\n",
    "  for i in range(0,org_imgs.shape[0]):\n",
    "    x_img = cv2.resize(org_imgs[i], dsize=(target_size, target_size), interpolation=cv2.INTER_CUBIC)\n",
    "    x_imgs.append(x_img)\n",
    "  return np.array(x_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 1s 31us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 30s 1us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 14s 3us/step\n"
     ]
    }
   ],
   "source": [
    "(images, labels), (_, _) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['0','1','2','3','4','5','6','7','8','9']\n",
    "NUM_OF_CLASS = len(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_zoomed_in = img_resize(images, 48)\n",
    "images_zoomed_in = images_zoomed_in.reshape(images_zoomed_in.shape[0], images_zoomed_in.shape[1], images_zoomed_in.shape[2], 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = images_zoomed_in[0:50000]\n",
    "x_test = images_zoomed_in[50000:60000]\n",
    "\n",
    "y_train = labels[0:50000]\n",
    "y_test = labels[50000:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_SIZE = x_train.shape[0]\n",
    "TEST_DATA_SIZE = x_test.shape[0]\n",
    "\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "TEST_BATCH_SIZE = 10\n",
    "\n",
    "train_step = int(TRAIN_DATA_SIZE/TRAIN_BATCH_SIZE)\n",
    "test_step = int(TEST_DATA_SIZE/TEST_BATCH_SIZE)\n",
    "\n",
    "EPOCHS_NUM = 3\n",
    "\n",
    "BASIC_TARGET_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = np_utils.to_categorical(y_train, NUM_OF_CLASS)\n",
    "y_test_oh = np_utils.to_categorical(y_test, NUM_OF_CLASS)\n",
    "\n",
    "x_train_3d = np.concatenate((x_train, x_train), axis=3)\n",
    "x_train_3d = np.concatenate((x_train_3d, x_train), axis=3)\n",
    "x_test_3d = np.concatenate((x_test, x_test), axis=3)\n",
    "x_test_3d = np.concatenate((x_test_3d, x_test), axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_self_gen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_self_gen.fit(x_train_3d)\n",
    "train_self_batches = train_self_gen.flow(x_train_3d, y_train_oh, batch_size=TRAIN_BATCH_SIZE, shuffle=False, seed=10)\n",
    "\n",
    "test_self_gen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_self_gen.fit(x_train_3d)\n",
    "test_self_batches = test_self_gen.flow(x_test_3d, y_test_oh, batch_size=TEST_BATCH_SIZE, shuffle=False, seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vgg16 data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vgg_3d = keras.applications.vgg16.preprocess_input(x_train_3d)\n",
    "x_test_vgg_3d = keras.applications.vgg16.preprocess_input(x_test_3d)\n",
    "\n",
    "train_vgg_batches = ImageDataGenerator().flow(x_train_vgg_3d, y_train_oh, batch_size=TRAIN_BATCH_SIZE, shuffle=False, seed=10)\n",
    "test_vgg_batches = ImageDataGenerator().flow(x_test_vgg_3d, y_test_oh, batch_size=TEST_BATCH_SIZE, shuffle=False, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create/tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_model(size, num_of_class):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', input_shape=(size,size,3)))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same'))  \n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2))    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))    \n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2))    \n",
    "    model.add(Dropout(0.5))    \n",
    "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(192, (1, 1),padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(num_of_class, (1, 1), padding='valid'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model(name, num=0,size=224, channel=1):\n",
    "    if name=='vgg16':\n",
    "        base_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(size,size,channel) )\n",
    "    elif name=='inception_v3':\n",
    "        base_model = keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(size,size,channel))\n",
    "    else:\n",
    "        return\n",
    "    x = base_model.output\n",
    "    predictions = Dense(NUM_OF_CLASS, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "      if i < len(base_model.layers) - num:\n",
    "        layer.trainable = False\n",
    "      else:\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_batches, test_batches, model_type, num_of_train_layer, optimizer, learning_rate, epoch):\n",
    "  \n",
    "  model_file_name = model_type + '.' + str(num_of_train_layer) + '-' + optimizer + '-' + str(learning_rate) + '-' + str(epoch) + '.{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.h5'\n",
    "  checkpoint = ModelCheckpoint(model_file_name, monitor='val_loss', verbose=1, save_best_only= True, mode='auto')\n",
    "  \n",
    "  if model_type == 'self':\n",
    "    model = self_model(48, NUM_OF_CLASS)\n",
    "  elif model_type == 'vgg16':\n",
    "    model = keras_model('vgg16', size=48, channel = 3, num=num_of_train_layer)\n",
    "  \n",
    "  if optimizer == 'SGD':\n",
    "    model.compile(SGD(lr=learning_rate), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  else:\n",
    "    model.compile(Adam(lr=learning_rate), loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  \n",
    "  h = model.fit_generator(train_batches, steps_per_epoch=train_step, validation_data=test_batches, callbacks=[checkpoint], validation_steps=test_step, epochs=epoch, verbose=1)\n",
    "  \n",
    "  return model, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(train_batches, test_batches, model_type = 'self', range_num_of_train_layer = 0, optimizer_list = ['Adam'], learning_rate_list = [0.0001], epoch_list = [2]):\n",
    "  \n",
    "  if model_type == 'self':\n",
    "    range_num_of_train_layer = 0\n",
    "\n",
    "  for num_of_train_layer in range(0, range_num_of_train_layer + 1):\n",
    "    for optimizer in optimizer_list:\n",
    "      for learning_rate in learning_rate_list:\n",
    "        for epoch in epoch_list:\n",
    "          print('current variables:' + model_type + '.' + str(num_of_train_layer) + '-' + optimizer + '-' + str(learning_rate) + '-' + str(epoch) )\n",
    "          model, h = train_model(train_batches, test_batches, model_type, num_of_train_layer, optimizer, learning_rate, epoch)\n",
    "          print('current variables:' + model_type + '.' + str(num_of_train_layer) + '-' + optimizer + '-' + str(learning_rate) + '-' + str(epoch) )\n",
    "          plot_model(h)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model(train_batches, test_batches, 'self', range_num_of_train_layer = 1, optimizer_list = ['Adam', 'SGD'], learning_rate_list = [0.001, 0.01, 0.1, 0.3], epoch_list = [20, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model(train_vgg_batches, test_vgg_batches, 'vgg16', range_num_of_train_layer = 3, optimizer_list = ['Adam', 'SGD'], learning_rate_list = [0.001, 0.01, 0.1, 0.3], epoch_list = [20, 50, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(model_folder_path):\n",
    "  \n",
    "  fileNames = os.listdir( model_folder_path )\n",
    "  best_model_path = {}\n",
    "  model_info = {}\n",
    "  \n",
    "  for i in range(0, len(fileNames)):\n",
    "    if '.h5' in fileNames[i]:\n",
    "      model_type = fileNames[i].split('.')[0]\n",
    "      suf = fileNames[i].split('-')[-1]\n",
    "      loss = float(suf[0:len(suf)-3])\n",
    "      if model_type in model_info:\n",
    "        if loss < model_info[model_type]:\n",
    "          model_info[model_type] = loss\n",
    "          best_model_path[model_type] = fileNames[i]\n",
    "      else:\n",
    "        model_info[model_type] = loss\n",
    "        best_model_path[model_type] = fileNames[i]\n",
    "  \n",
    "  models = []\n",
    "  for k, v in best_model_path.items():\n",
    "    models.append(load_model(model_folder_path + '/' + v))\n",
    "    \n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = get_best_models(os.listdir(os.path.abspath('./models')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_voting_pred(best_models):\n",
    "  \n",
    "  pred_prob = []\n",
    "  acc = []\n",
    "  for i in range(0,len(best_models)):\n",
    "    pred_prob.append(best_models[i].predict_generator(test_batches,steps=test_step, verbose=1))\n",
    "    acc.append(accuracy_score(y_test, get_labels(pred_prob[i])))\n",
    "\n",
    "  print('original accuracy:')\n",
    "  print(acc)\n",
    "  acc = np.array(acc)\n",
    "  pred_prob = np.array(pred_prob)\n",
    "  acc_sum = np.sum(acc, axis = 0)\n",
    "  model_weights = acc / acc_sum\n",
    "  \n",
    "  ens_pred = np.dot(pred_prob.T, model_weights).T\n",
    "  \n",
    "  return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_pred_1 = weight_voting_pred(best_models)\n",
    "print('ensemble accuracy:')\n",
    "print(accuracy_score(y_test, get_labels(ens_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

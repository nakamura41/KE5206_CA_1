{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e05535dd-21d5-4b69-9c52-0eec04aec10a",
    "_uuid": "bde6f13675f847c579ea8d6481e1f34ba62a8741"
   },
   "source": [
    "# Predicting hart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8e33d119-031c-4c2c-b515-abeb55c93d33",
    "_uuid": "c72059f9b5a2921746f0d4572493a86b8a80f13d"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Relevance. According to a report of the American Heart Association Statistics (2016), heart disease is the leading cause of death for both men and women and responsible for 1 in every 4 deaths, even modest improvements in prognostic models of heart event and complications could save literally hundreds of lives and help to significantly reduce the cost of health care services, medications, and lost productivity.\n",
    "\n",
    "\n",
    "file:///C:/Users/User/Downloads/350-904-1-PB%20(1).pdf\n",
    "http://inpressco.com/wp-content/uploads/2017/10/Paper271842-1853.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d9e16edc-196c-484e-81b4-4eedb6ca272b",
    "_uuid": "986a1b696e4150bf75309c776ee7bbf10b8a46d0"
   },
   "source": [
    "## Methods \n",
    "\n",
    "Deep neural networks (DNN) represents a set of modern machine learning (ML) models that have gain widespread recognition because they were behind the first FDA (US food and drug administration) approved machine learning application in healthcare; to be approved it had to pass tests to show it can produce results at least as accurately as humans are currently able to. Recently, such ML models were also used to detect with cardiologist-level accuracy 14 types of arrhythmias (sometime life-threatening heart beats) form ECG-electrocardiogram signals generated by wearable monitors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "765b1f2f-5b6c-4b3e-a1f6-66d61a8c9aea",
    "_uuid": "22795be3a63f7e30c2c2d2a2fc20ced2eac392e9"
   },
   "source": [
    "## Original contribution\n",
    "\n",
    "Studies exploring the potential of this technology for the prognosis of cardiovascular events/complications from risk factors have been limited; events/complications are, for example, coronary artery disease, stroke and congestive heart failure, and risk factors are those established by the American College of Cardiology/American Heart Association (ACC/AHA) such as age, high blood pressure, high LDL cholesterol, and smoking and others, such as, systolic blood pressure variability, kidney disease, and ethnicity. \n",
    "\n",
    "Most of previous studies have either used logistic regression or classical machine learning algorithms such as random forest, gradient boosting and neural networks (non-deep); in addition, comparison studies of the cited algorithms with deep learning models in the specific prognosis context under consideration are not readily available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb0e279a-85db-4ff4-9e19-d0afd7678579",
    "_uuid": "425fc124a20f4d6392ee1840a9df54c84844e81b"
   },
   "source": [
    "## Research objectives\n",
    "\n",
    "Establish the relative performance of deep learning models, such as deep belief networks and convolutional neural networks, and ensembles with respect to classical machine learning algorithms (including logistic regression) using cases studies built from well-known heart disease data sets such as the Cleveland set available from the UCI repository. Research questions of interest are, for example, for what would be the threshold of sample size in heart disease studies where the more complex but potentially more effective deep learning models would be recommended?, would ensembles of machine learning models be able to provide more robust predictions as it has been the case in other knowledge domains?, does the ACC/AHA list of eight risk factors should be updated with other genetic or lifestyle factors?. The deep learning models will be implemented in Tensorflow (originally from Google, now open source) and healthcare.ai, an open source that facilitate the development of machine learning in healthcare, with the prevision that can handle so called big data by using the Hadoop/Spark platform.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8e8b8c1f-ad72-40d7-9508-81e528bd46bb",
    "_uuid": "7c1d8044361a7931707cddb7492fd68e48016b44"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE #for SMOTE -> install package using: conda install -c conda-forge imbalanced-learn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "235d000d-7d9a-4dc9-a5e9-a8a475b9896f",
    "_uuid": "8a0522ef39bfa8d1a18cd75f8202e583661d759b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\utils.py:81: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access Timestamp as pandas.Timestamp\n",
      "  pd.tslib.Timestamp,\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\ggplot\\stats\\smoothers.py:4: FutureWarning: The pandas.lib module is deprecated and will be removed in a future version. These are private functions and can be accessed from pandas._libs.lib instead\n",
      "  from pandas.lib import Timestamp\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\Users\\RyanT\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import pylab as pl\n",
    "from itertools import cycle\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "4e8a77fd-b674-46fb-b480-ad6c0b3a5d7f",
    "_uuid": "d12c643fdd7a7479fdc3d5d6e65567e11a34203d",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-79a67b8b40bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sex'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'trestbps'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'chol'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fbs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'restecg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'thalach'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'exang'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'oldpeak'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'slope'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ca'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'thal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PCA Dataset original.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PCA Dataset original.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "features_list = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','class']\n",
    "dataset1=pd.read_csv(\"PCA Dataset original.csv\")\n",
    "dataset2=pd.read_csv(\"PCA Dataset original.csv\")\n",
    "\n",
    "dataset1 = dataset1.convert_objects(convert_numeric=True)\n",
    "dataset1.astype('float')\n",
    "\n",
    "\n",
    "\n",
    "# SVM requires that each data instance is represented as a vector of real numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### count missing value in terms of colunms #######\n",
    "#dataset.shape[0] - dataset.count()\n",
    "dataset1.isnull()\n",
    "dataset1.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some values and no duplicated data in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkforoutlier(df):\n",
    "    outliersnumbers = 0\n",
    "    for column in df:\n",
    "        for number in df[column]:\n",
    "            if number < np.percentile(\n",
    "                df[column], 25)-(np.percentile(\n",
    "                df[column], 75)-np.percentile(\n",
    "                df[column], 25)) or number > np.percentile(\n",
    "                df[column], 75)+(np.percentile(\n",
    "                df[column], 75)-np.percentile(\n",
    "                df[column], 25)):\n",
    "                    print(\"outlier: \", number)\n",
    "                    outliersnumbers += 1\n",
    "    return outliersnumbers, 'outliers. That is', round(float(outliersnumbers)/float(len(df[column]))*100, 0), 'percent of the total list'\n",
    "\n",
    "print(checkforoutlier(dataset1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset1.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminary description of the data.\n",
    "Box plots and histograms were used for continuous and categorical variables.\n",
    "Basic statistics are also available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3f859aa0-fc03-4325-9378-48012735d14b",
    "_uuid": "9b92fe125729a3cfafb6752fdef27f84d982ddcf",
    "collapsed": true
   },
   "source": [
    "# Continuous variables \n",
    "basic statistics + Box plots + histograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4d835113-542b-41ad-9654-6b59ddd99954",
    "_uuid": "de5b369e4cdeb022b60051ee868a5539d9c01429",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## basic statistic descriptions\n",
    "continuas=[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "dataset1[continuas].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## age histogram\n",
    "ag= np.array(dataset1['age']) \n",
    "plt.hist(ag, bins = 6) \n",
    "plt.title(\"age\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trestbps histogram\n",
    "tp= np.array(dataset1['trestbps']) \n",
    "plt.hist(tp, bins=6) \n",
    "plt.title(\"Trestbps:blood pressure\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chol histogram\n",
    "ch= np.array(dataset1['chol'])\n",
    "sns.distplot(ch, rug=True, hist=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## thalach histogram and distribution \n",
    "tl= np.array(dataset1['thalach'])\n",
    "sns.distplot(tl, rug=True, hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## oldpeak histogram and distribution \n",
    "op= np.array(dataset1['oldpeak'])\n",
    "sns.distplot(op, rug=True, hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## thalach histogram\n",
    "ca= np.array(dataset1['ca']) \n",
    "plt.hist(tp, bins=4) \n",
    "plt.title(\"histogram of ca\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "992069e9-2599-4b46-84a2-6c5e8abc9e08",
    "_uuid": "25d656add052b4b9c70dc409b1103ada4fe1a9db",
    "collapsed": true
   },
   "source": [
    "### Categorical variables \n",
    "##### Histograms + basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex: sex (1 = male; 0 = female) \n",
    "tempo5 = dataset1['sex']\n",
    "tempo5.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "165acbd9-e9f2-49cc-a163-a1a714b231b2",
    "_uuid": "b2ca56f6aa50c9cbe163fda300dc05dc77be5f95"
   },
   "outputs": [],
   "source": [
    "#Fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "tempo6 = dataset1['fbs']\n",
    "tempo6.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94cb7d70-07ea-4dc8-a686-34e40e3a88dc",
    "_uuid": "6f86e31d259f4c6982fbf5d8761edab341602669"
   },
   "outputs": [],
   "source": [
    "#Slope: the slope of the peak exercise ST segment  \n",
    "#Value 1: upsloping \n",
    "#Value 2: flat \n",
    "#Value 3: downsloping\n",
    "tempo7 = dataset1['slop']\n",
    "tempo7.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a689218-4173-4169-90e6-ba2640dde924",
    "_uuid": "d7e4dceb43a2a6c598aab5b796a8f64fabb3a0e3"
   },
   "outputs": [],
   "source": [
    "#Cp: chest pain type\n",
    "#Value 1: typical angina \n",
    "#Value 2: atypical angina \n",
    "#Value 3: non-anginal pain \n",
    "#Value 4: asymptomatic \n",
    "tempo8 = dataset1['cp']\n",
    "tempo8.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "66a9c02d-11b8-41a8-a97e-bca3c7278939",
    "_uuid": "dee8d81e3efe783c3a2dc6fea7432fa2a2bd0a95"
   },
   "outputs": [],
   "source": [
    "#Exang: exercise induced angina (1 = yes; 0 = no) \n",
    "tempo9 = dataset1['exang']\n",
    "tempo9.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "235ab7fb-3a08-4c8e-bd2c-4ef8e6d1129e",
    "_uuid": "ad4719dc5cf0476037d683dee63f728d0fec4ed8"
   },
   "outputs": [],
   "source": [
    "#Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "tempo10 = dataset1['thal']\n",
    "tempo10.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a1ab9dce-1b82-4556-8dee-6a6f50303389",
    "_uuid": "cbc6ce5594ddb4b57821ddff0770176499a24c17"
   },
   "outputs": [],
   "source": [
    "#Restecg: resting electrocardiographic results \n",
    "#Value 0: normal \n",
    "#Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n",
    "#Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    "tempo11 = dataset1['restecg']\n",
    "tempo11.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7140b7fd-b218-4ba8-b78a-b2282104c6d5",
    "_uuid": "7d82c9ede07d463dc884534cafbed825b948a975"
   },
   "outputs": [],
   "source": [
    "#Class: diagnosis of heart disease (angiographic disease status) \n",
    "#Value 0: < 50% diameter narrowing (Healthy)\n",
    "#Value 1: > 50% diameter narrowing (Sick)\n",
    "tempo12 = dataset1['pred_attribute']\n",
    "tempo12.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "p = ggplot(dataset1,aes(x='pred_attribute'))\n",
    "p + geom_bar()+facet_wrap('sex')  #### relationship between pred_attribute and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### relationship between age and trestbps\n",
    "p = ggplot(dataset1,aes(x='age'))\n",
    "p+geom_histogram()+facet_wrap('pred_attribute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=ggplot(dataset1,aes(x='age',y='trestbps'))\n",
    "p+geom_line()+facet_wrap('pred_attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=ggplot(dataset1,aes(x='age',y='thalach'))\n",
    "p+geom_line()+facet_wrap('pred_attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ggplot(dataset1,aes(x='pred_attribute'))\n",
    "p + geom_bar()+facet_wrap('cp') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = ggplot(dataset1,aes(x='pred_attribute'))\n",
    "# p + geom_bar()+facet_wrap('fbs') \n",
    "# ggplot(data)+geom_histogram(aes(x=price, fill=cut), position=\"dodge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing\n",
    " outlier and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Boxplots of all continuous variable\n",
    "# continuas=[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"ca\"]\n",
    "# dataset[continuas].boxplot(return_type='axes', figsize=(12,8))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### show the patients whose trestbps above 180\n",
    "# print(dataset[dataset['trestbps']>=180])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### show the patients whose chol above 370\n",
    "# print(dataset[dataset['chol']>400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### show the patients whose thalach below 180\n",
    "# print(dataset[dataset['thalach']<90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### show the patients whose thalach below 180\n",
    "# print(dataset[dataset['oldpeak']>5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### delete outliers by mean\n",
    "# dataset1=dataset1.drop([83,126,188,201,231,48,121,152,181,175,245,91,123])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#from sklearn.cluster import KMeans\n",
    "#data=dataset1\n",
    "#estimator = KMeans(n_clusters=3)\n",
    "#estimator.fit(data)#聚类\n",
    "#label_pred = estimator.labels_ #获取聚类标签\n",
    "#centroids = estimator.cluster_centers_ \n",
    "#inertia = estimator.inertia_ \n",
    "#mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr'] \n",
    "#color = 0\n",
    "#j = 0 \n",
    "#for i in label_pred:\n",
    "    #plt.plot([data[j:j+1,0]], [data[j:j+1,1]], mark[i], markersize = 5)\n",
    "    #j +=1\n",
    "#plt.show()''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e1b70405-cc10-4b24-a5a5-38ac9c7ebf0f",
    "_uuid": "16e3dd8adbaaa08120e1ab1e32dd4204e843c60a"
   },
   "source": [
    "### 2. Define training and test samples. \n",
    "\n",
    "The Cleveland data set available from the UCI repository has 303 samples; the training and test data sets were randomly selected with 30% of the original data set corresponding to the test data set.  The relative proportions of the classes of interest (disease/no disease) in both sets were checked to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b689622a-a475-40a8-bd33-e2b5e018d528",
    "_uuid": "2e13a97aaee5c269ff8f21fd7b66135927b66156"
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing:\n",
    "\n",
    "dataset1.dropna(inplace=True, axis=0, how=\"any\")\n",
    "X=dataset1.loc[:, \"age\":\"thal\" ]\n",
    "Y=dataset1[\"pred_attribute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2dacb7c2-6d4e-4a17-b5cb-d6ecd821c923",
    "_uuid": "c3355ae680b60b0daa713153f05b3709193af7f6"
   },
   "outputs": [],
   "source": [
    "# evaluate the model by splitting into train and test sets  #Edit by ryan, we aim to do 3 traditional sets in the end, this first split is 80/20\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5065d9ba-be53-4431-bb86-152ee1673550",
    "_uuid": "00229a787842594dee105c79de5871548613a045"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "list1 = []\n",
    "for i in labels_train:\n",
    "    list1.append(i)\n",
    "counter=collections.Counter(list1)\n",
    "print(counter)\n",
    "\n",
    "list2 = []\n",
    "for i in labels_test:\n",
    "    list2.append(i)\n",
    "counter=collections.Counter(list2)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "print(len(features_train)/(len(features_train)+ len(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "validated set. We will check this assumption by comparing the scores on a cross-validated set vs the simple split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_cross, features_test_cross, labels_train_cross, labels_test_cross = train_test_split(X, Y, test_size=0.15, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE for SVM - Balancing only on the training set, not the validation set  [This is for the traditional training -not the cross validated one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further divide the 'traditional' non-cross set into training 80/20  for pure training and cross validation  \n",
    "features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate = train_test_split(features_train, labels_train, test_size = .15, random_state=0)\n",
    "\n",
    "sm = SMOTE(random_state=0, ratio = 1.0, kind= 'svm' )\n",
    "#x_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n",
    "features_train_oversampled, labels_train_oversampled = sm.fit_sample(features_train_notoversampled, labels_train_notoversampled)\n",
    "\n",
    "#re-enter into original variables\n",
    "##features_train = features_train_oversampled\n",
    "##labels_train = labels_train_oversampled\n",
    "\n",
    "#Below 2 lines if we want to want to force the array back into dataframe    \n",
    "##features_train = pd.DataFrame(features_train_oversampled,columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slop\",\"ca\",\"thal\"])\n",
    "##labels_train = pd.DataFrame(labels_train_oversampled,columns=[\"pred_attribute\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 make pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing.data import QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM needs a scalar: https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n",
    "\n",
    "MinMaxScaler rescales the data set such that all feature values are in the range [0, 1]. However, this scaling compress all inliers in the narrow ranges. Minmaxscalar also suffers from the presence of large outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous scaler, the centering and scaling statistics of Robust scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar: for both features most of the transformed values lie in a [-2, 3] range as seen in the zoomed-in figure. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Robust_scaler = preprocessing.RobustScaler(quantile_range=(25, 75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuantileTransformer applies a non-linear transformation such that the probability density function of each feature will be mapped to a uniform distribution. In this case, all the data will be mapped in the range [0, 1], even the outliers which cannot be distinguished anymore from the inliers. QuantileTransformer has an additional output_distribution parameter allowing to match a Gaussian distribution instead of a uniform distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantile_scalar = preprocessing.QuantileTransformer(output_distribution='normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting features\n",
    "\n",
    "SelectKBest is way to select the most powerfull features. It is also possible to do this manually, in my experience this may improve the results drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "skb = SelectKBest(k = 2)\n",
    "skb3 = SelectKBest(k = 3)\n",
    "skb4 = SelectKBest(k = 4)\n",
    "skb7 = SelectKBest(k = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 selecting a Kernel & it's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### start with RGB, because we don't have a lot of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice visualisations:\n",
    "    \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html \n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import data\n",
    "X = np.array(features_test)\n",
    "y = np.array(labels_test)\n",
    "class_names = np.array(['No disease', 'Disease 1', 'Disease 2', 'Disease 3', 'Disease 4'],\n",
    "      dtype='<U10')\n",
    "\n",
    "# # Split the data into a training set and a test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# # Run classifier, using a model that is too regularized (C too low) to see\n",
    "# # the impact on the results\n",
    "# classifier = svm.SVC(kernel='linear', C=0.01)\n",
    "# y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import grid_search\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "def checkmetrics(pred, labels_test, name):\n",
    "    print('The accuracy is of a', name, 'is: ', accuracy_score(pred, labels_test))\n",
    "    # print 'if everyone had 0 score: ', float(float(len(pred))-float(numberpoi))/float(len(pred))\n",
    "#     matrix = confusion_matrix(labels_test, pred)\n",
    "#     print('There are', matrix[0][0], 'healthy people correctly identified vs', matrix[2][2] +matrix[3][3] +matrix[4][4] +matrix[1][1], 'sick ones. See:\\n', matrix)\n",
    "    print(classification_report(pred, labels_test))\n",
    "    \n",
    "   \n",
    "\n",
    "#     print('precision score:', precision_score( pred, labels_test))\n",
    "#     if precision_score(pred, labels_test) < recall_score(pred, labels_test):\n",
    "#         print('precision < recall, so higher chance on POIs get identified, but also more false positives')\n",
    "#     if precision_score(pred, labels_test) > recall_score(pred, labels_test):\n",
    "#         print('precision > recall, so lower chance on POIs get identified, but also less false positives')\n",
    "#     print('f1 score: ', f1_score(pred, labels_test), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONE\n",
    "\n",
    "clf = SVC(kernel=\"rbf\")\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE for training- Validation - support vector machine, Radial Basis Function')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels_validate, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Validation Confusion matrix, without normalization')\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Validation Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE for training - Test - support vector machine, Radial Basis Function')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels_test, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONE\n",
    "\n",
    "clf = SVC(kernel=\"rbf\")\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE for training- Validation - support vector machine, Radial Basis Function')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels_validate, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Validation Confusion matrix, without normalization')\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Validation Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE for training - Test - support vector machine, Radial Basis Function')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(labels_test, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other ones seem to be just as bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  Pipeline(steps=[('scaling', scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate support vector machine, Radial Basis Function scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',Robust_scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate support vector machine, Radial Basis Function Robust scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling', Quantile_scalar), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate support vector machine, Radial Basis Function Normal scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate support vector machine, Radial Basis Function scaled & selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  Pipeline(steps=[('scaling', scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test support vector machine, Radial Basis Function scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',Robust_scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test support vector machine, Radial Basis Function Robust scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling', Quantile_scalar), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test support vector machine, Radial Basis Function Normal scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test support vector machine, Radial Basis Function scaled & selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  Pipeline(steps=[('scaling', scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate support vector machine, Radial Basis Function scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',Robust_scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate support vector machine, Radial Basis Function Robust scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling', Quantile_scalar), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate support vector machine, Radial Basis Function Normal scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate support vector machine, Radial Basis Function scaled & selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  Pipeline(steps=[('scaling', scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test support vector machine, Radial Basis Function scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',Robust_scaler), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test support vector machine, Radial Basis Function Robust scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling', Quantile_scalar), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test support vector machine, Radial Basis Function Normal scaled')\n",
    "\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"rbf\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test support vector machine, Radial Basis Function scaled & selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying poly\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"poly\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate -support vector machine, Poly, scaled & clustered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying poly\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"poly\"))])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test -support vector machine, Poly, scaled & clustered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying poly\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"poly\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -support vector machine, Poly, scaled & clustered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying poly\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), ('support vector machine', SVC(kernel=\"poly\"))])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test -support vector machine, Poly, scaled & clustered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to lay far too much weight on the first category due to their high number. \n",
    "\n",
    "##### Try other peoples classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used some ideas from http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "#linear kernel\n",
    "def svm_linear(features_train, features_test, labels_train, labels_test):\n",
    "\n",
    "    clf = svm.SVC(\n",
    "        C=1.0, \n",
    "        kernel='linear', \n",
    "        probability=False, \n",
    "        shrinking=True, \n",
    "        tol=1e-3, \n",
    "        verbose=False, \n",
    "        max_iter=-1, \n",
    "        decision_function_shape=None,\n",
    "        random_state=None)\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    print(\"Kernel: Linear\")\n",
    "    print(\"Performance: \"  + str(clf.score(features_test, labels_test)))\n",
    "    print(\"\")\n",
    "    return pred\n",
    "\n",
    "#polynomial kernel from degrees 2 to 5\n",
    "def svm_poly(features_train, features_test, labels_train, labels_test):\n",
    "\n",
    "    for d in [2, 3, 4, 5]:\n",
    "\n",
    "        clf = svm.SVC(\n",
    "            C=1.0,\n",
    "            kernel='poly', \n",
    "            degree=d,\n",
    "            gamma='auto',\n",
    "            coef0=0.0,\n",
    "            probability=False,\n",
    "            shrinking=True,\n",
    "            tol=1e-3,\n",
    "            verbose=False,\n",
    "            max_iter=400000,\n",
    "            decision_function_shape=None,\n",
    "            random_state=None)\n",
    "        clf.fit(features_train, labels_train)\n",
    "        pred = clf.predict(features_test)\n",
    "        print(\"Kernel: Polynomial\")\n",
    "        print(\"Degree: \" + str(d))\n",
    "        print(\"Performance: \"  + str(clf.score(features_test, labels_test)))\n",
    "        print(\"\")\n",
    "    return pred\n",
    "\n",
    "#radial basis function kernel\n",
    "def svm_rbf(features_train, features_test, labels_train, labels_test):\n",
    "\n",
    "    clf = svm.SVC(\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        gamma='auto',\n",
    "        probability=False,\n",
    "        shrinking=True,\n",
    "        tol=1e-3,\n",
    "        verbose=False,\n",
    "        max_iter=-1,\n",
    "        decision_function_shape=None,\n",
    "        random_state=None)\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    print(\"Kernel: Radial Basis Function\")\n",
    "    print(\"Performance: \"  + str(clf.score(features_test, labels_test)))\n",
    "    print(\"\")\n",
    "    return pred\n",
    "\n",
    "#sigmoid function kernel\n",
    "def svm_sigmoid(features_train, features_test, labels_train, labels_test):\n",
    "\n",
    "    clf = svm.SVC(\n",
    "        C=1.0,\n",
    "        kernel='sigmoid',\n",
    "        gamma='auto',\n",
    "        coef0=0.0,\n",
    "        probability=False,\n",
    "        shrinking=True,\n",
    "        tol=1e-3,\n",
    "        verbose=False,\n",
    "        max_iter=-1,\n",
    "        decision_function_shape=None,\n",
    "        random_state=None)\n",
    "    clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    print(\"Kernel: Sigmoid\")\n",
    "    print(\"Performance: \"  + str(clf.score(features_test, labels_test)))\n",
    "    print(\"\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a94b089-0ab5-495b-a2ed-434001cf61af",
    "_uuid": "92ce1308415ee9775a6f29d19d5eeb4b2c6d959a"
   },
   "outputs": [],
   "source": [
    "#Run SVM with a linear kernel\n",
    "pred = svm_linear(features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate - linearSVM, from function')\n",
    "\n",
    "#Run SVM with a polynomial kernel\n",
    "pred = svm_poly(features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate - polySVM, from function')\n",
    "\n",
    "#Run SVM with a radial basis function kernel\n",
    "pred = svm_rbf(features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate - RBFSVM, from function')\n",
    "\n",
    "#Run SVM with a sigmoid kernel\n",
    "pred = svm_sigmoid(features_train_notoversampled, features_validate, labels_train_notoversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'Without SMOTE - Validate - SIGSVM, from function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SVM with a linear kernel\n",
    "pred = svm_linear(features_train_notoversampled, features_test, labels_train_notoversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test - linearSVM, from function')\n",
    "\n",
    "#Run SVM with a polynomial kernel\n",
    "pred = svm_poly(features_train_notoversampled, features_test, labels_train_notoversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test -polySVM, from function')\n",
    "\n",
    "#Run SVM with a radial basis function kernel\n",
    "pred = svm_rbf(features_train_notoversampled, features_test, labels_train_notoversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test -RBFSVM, from function')\n",
    "\n",
    "#Run SVM with a sigmoid kernel\n",
    "pred = svm_sigmoid(features_train_notoversampled, features_test, labels_train_notoversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'Without SMOTE - Test -SIGSVM, from function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SVM with a linear kernel\n",
    "pred = svm_linear(features_train_oversampled, features_validate, labels_train_oversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -linearSVM, from function')\n",
    "\n",
    "#Run SVM with a polynomial kernel\n",
    "pred = svm_poly(features_train_oversampled, features_validate, labels_train_oversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -polySVM, from function')\n",
    "\n",
    "#Run SVM with a radial basis function kernel\n",
    "pred = svm_rbf(features_train_oversampled, features_validate, labels_train_oversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -RBFSVM, from function')\n",
    "\n",
    "#Run SVM with a sigmoid kernel\n",
    "pred = svm_sigmoid(features_train_oversampled, features_validate, labels_train_oversampled, labels_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -SIGSVM, from function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SVM with a linear kernel\n",
    "pred = svm_linear(features_train_oversampled, features_test, labels_train_oversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test -linearSVM, from function')\n",
    "\n",
    "#Run SVM with a polynomial kernel\n",
    "pred = svm_poly(features_train_oversampled, features_test, labels_train_oversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test -polySVM, from function')\n",
    "\n",
    "#Run SVM with a radial basis function kernel\n",
    "pred = svm_rbf(features_train_oversampled, features_test, labels_train_oversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test -RBFSVM, from function')\n",
    "\n",
    "#Run SVM with a sigmoid kernel\n",
    "pred = svm_sigmoid(features_train_oversampled, features_test, labels_train_oversampled, labels_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test -SIGSVM, from function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Linear one is performing a little bit better here, there seems to be a very big focus on the non-disease. Still not good: try some automatic tuning: trying gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Can we use gridsearch for feature selection?\n",
    "\n",
    "# still not good: try some automatic tuning:\n",
    "# trying gridsearch\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler), (\"Grid\", grid)])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'No SMOTE - Validate - support vector machine, with gridsearch')\n",
    "\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"Grid\", grid)])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'No SMOTE - Validate - support vector machine, with gridsearch & only the best 2 features')\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb7), (\"Grid\", grid)])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'No SMOTE - Validate - support vector machine, with gridsearch & only the best 7 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Can we use gridsearch for feature selection?  \n",
    "\n",
    "# still not good: try some automatic tuning:\n",
    "# trying gridsearch\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler), (\"Grid\", grid)])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - Test - support vector machine, with gridsearch')\n",
    "\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"Grid\", grid)])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - Test - support vector machine, with gridsearch & only the best 2 features')\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb7), (\"Grid\", grid)])\n",
    "clf.fit(features_train_notoversampled, labels_train_notoversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'No SMOTE - Test - support vector machine, with gridsearch & only the best 7 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Can we use gridsearch for feature selection?\n",
    "\n",
    "# still not good: try some automatic tuning:\n",
    "# trying gridsearch\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler), (\"Grid\", grid)])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -support vector machine, with gridsearch')\n",
    "\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"Grid\", grid)])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -support vector machine, with gridsearch & only the best 2 features')\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb7), (\"Grid\", grid)])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_validate)\n",
    "checkmetrics(pred, labels_validate, 'With SMOTE - Validate -support vector machine, with gridsearch & only the best 7 features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Can we use gridsearch for feature selection?\n",
    "\n",
    "# still not good: try some automatic tuning:\n",
    "# trying gridsearch\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler), (\"Grid\", grid)])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test - support vector machine, with gridsearch')\n",
    "\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"Grid\", grid)])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test - support vector machine, with gridsearch & only the best 2 features')\n",
    "\n",
    "parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "grid = grid_search.GridSearchCV(svr, parameters)\n",
    "clf =  Pipeline(steps=[('scaling',scaler),(\"SKB\", skb7), (\"Grid\", grid)])\n",
    "clf.fit(features_train_oversampled, labels_train_oversampled)\n",
    "pred = clf.predict(features_test)\n",
    "checkmetrics(pred, labels_test, 'With SMOTE - Test - support vector machine, with gridsearch & only the best 7 features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we were just unlucky, had a difficult split? We have a relatively small dataset. Therefore, we should do our feature selection based on a cross-validated set. Let's check if the scoring is the same on a cross validated set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have an relatively small dataset. Therefore, we should do our feature selection based on a cross-\n",
    "# validated set. Let's check if the scoring is the same on a cross validated set.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "# split into 5\n",
    "scores = cross_val_score(clf, features_train_cross, labels_train_cross, cv=5)\n",
    "                                            \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=0.2)\n",
    "# split into 5\n",
    "scores = cross_val_score(clf, features_train_cross, labels_train_cross, cv=5)\n",
    "                                            \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=0.1)\n",
    "# split into 5\n",
    "scores = cross_val_score(clf, features_train_cross, labels_train_cross, cv=5)\n",
    "                                            \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That accuracy seems a bit higher. We have to find a way to get the matrix from it and run it on \n",
    "# all the above methods (using gridsearch)\n",
    "\n",
    "# if that does not work, we need to somehow balance the data.\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "pred = cross_val_predict(clf, features_train_cross, labels_train_cross, cv=5)\n",
    "\n",
    "len(pred)\n",
    "# clf.fit(features_train, labels_train)\n",
    "# pred = clf.predict(features_test)\n",
    "# checkmetrics(pred, labels_test_cross, 'support vector machine, with gridsearch')\n",
    "\n",
    "# pred = cross_val_predict(clf, features_train, .target, cv=10)\n",
    "# # split into 5\n",
    "# scores = cross_val_score(clf, features_train_cross, labels_train_cross, cv=5)\n",
    "# clf.fit(features_train, labels_train)                                      \n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "# pred = clf.predict(features_test_cross)\n",
    "# checkmetrics(pred, labels_test_cross, 'using cross validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.names=names(heart.data)\n",
    "\n",
    "for (f in feature.names) {\n",
    "  if (class(heart.data[[f]])==\"factor\") {\n",
    "    levels <- unique(c(heart.data[[f]]))\n",
    "    heart.data[[f]] <- factor(heart.data[[f]],\n",
    "                   labels=make.names(levels))\n",
    "  }\n",
    "}\n",
    "set.seed(10)\n",
    "inTrainRows <- createDataPartition(heart.data$num,p=0.7,list=FALSE)\n",
    "trainData2 <- heart.data[inTrainRows,]\n",
    "testData2 <-  heart.data[-inTrainRows,]\n",
    "\n",
    "\n",
    "fitControl <- trainControl(method = \"repeatedcv\",\n",
    "                           number = 10,\n",
    "                           repeats = 10,\n",
    "                           ## Estimate class probabilities\n",
    "                           classProbs = TRUE,\n",
    "                           ## Evaluate performance using\n",
    "                           ## the following function\n",
    "                           summaryFunction = twoClassSummary)\n",
    "\n",
    "set.seed(10)\n",
    "gbmModel <- train(num ~ ., data = trainData2,\n",
    "                 method = \"gbm\",\n",
    "                 trControl = fitControl,\n",
    "                 verbose = FALSE,\n",
    "                 tuneGrid = gbmGrid,\n",
    "                 ## Specify which metric to optimize\n",
    "                 metric = \"ROC\")\n",
    "gbmPrediction <- predict(gbmModel, testData2)\n",
    "gbmPredictionprob <- predict(gbmModel, testData2, type='prob')[2]\n",
    "gbmConfMat <- confusionMatrix(gbmPrediction, testData2[,\"num\"])\n",
    "#ROC Curve\n",
    "AUC$gbm <- roc(as.numeric(testData2$num),as.numeric(as.matrix((gbmPredictionprob))))$auc\n",
    "Accuracy$gbm <- gbmConfMat$overall['Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(10)\n",
    "svmModel <- train(num ~ ., data = trainData2,\n",
    "                 method = \"svmRadial\",\n",
    "                 trControl = fitControl,\n",
    "                 preProcess = c(\"center\", \"scale\"),\n",
    "                 tuneLength = 8,\n",
    "                 metric = \"ROC\")\n",
    "svmPrediction <- predict(svmModel, testData2)\n",
    "svmPredictionprob <- predict(svmModel, testData2, type='prob')[2]\n",
    "svmConfMat <- confusionMatrix(svmPrediction, testData2[,\"num\"])\n",
    "#ROC Curve\n",
    "AUC$svm <- roc(as.numeric(testData2$num),as.numeric(as.matrix((svmPredictionprob))))$auc\n",
    "Accuracy$svm <- svmConfMat$overall['Accuracy'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FRU6Om846FCy"
   },
   "outputs": [],
   "source": [
    "!mkdir models2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1680,
     "status": "ok",
     "timestamp": 1526004488342,
     "user": {
      "displayName": "Ryan Tsan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "107656243123433916627"
     },
     "user_tz": -480
    },
    "id": "i1kFGCBO6Nug",
    "outputId": "b7fb1f8b-ccd1-457d-9684-2f32971efc75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\t RNN_LSTM_Keras_working.ipynb\r\n",
      "models2  RNN_LSTM_Keras_working-just_for_SGD_Momentum.ipynb\r\n",
      "Models2\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4083
    },
    "colab_type": "code",
    "id": "4Pipxp3mXlUq",
    "outputId": "59b29f67-3cd3-426a-cd9d-6194d22149ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryantsan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 48, 48, 1)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def img_resize(org_imgs, target_size):\n",
    "  x_imgs = []\n",
    "  for i in range(0,org_imgs.shape[0]):\n",
    "    x_img = cv2.resize(org_imgs[i], dsize=(target_size, target_size), interpolation=cv2.INTER_CUBIC)\n",
    "    x_imgs.append(x_img)\n",
    "  return np.array(x_imgs)\n",
    "\n",
    "\n",
    "def plot_model(model_details):\n",
    "\n",
    "    # Create sub-plots\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    # Summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_details.history['loss'])+1),model_details.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_details.history['val_loss'])+1),model_details.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_details.history['loss'])+1),len(model_details.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # Summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_details.history['acc'])+1),model_details.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_details.history['val_acc'])+1),model_details.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_details.history['acc'])+1),len(model_details.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Training parameters.\n",
    "batch_sizes = [64, 128]\n",
    "num_epochs = [25]\n",
    "learning_rates = [0.001, 0.01]\n",
    "optimizers_name = ['rmsprop', 'adam','sgd']\n",
    "num_classes = 10\n",
    "node_sizes = [128]\n",
    "\n",
    "# The data, shuffled and split between train and test sets.\n",
    "(images, labels), (_, _) = fashion_mnist.load_data()\n",
    "\n",
    "images_zoomed_in = img_resize(images, 48)\n",
    "images_zoomed_in = images_zoomed_in.reshape(images_zoomed_in.shape[0], images_zoomed_in.shape[1], images_zoomed_in.shape[2], 1).astype('float32')\n",
    "\n",
    "x_train = images_zoomed_in[0:50000]\n",
    "x_test = images_zoomed_in[50000:60000]\n",
    "\n",
    "y_train = labels[0:50000]\n",
    "y_test = labels[50000:60000]\n",
    "\n",
    "\n",
    "# x_train -= np.mean(x_train)\n",
    "# x_test -= np.mean(x_train)\n",
    "\n",
    "# x_train /= np.std(x_train)\n",
    "# x_test /= np.std(x_train)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "TRAIN_DATA_SIZE = x_train.shape[0]\n",
    "TEST_DATA_SIZE = x_test.shape[0]\n",
    "\n",
    "#TRAIN_BATCH_SIZE = 20\n",
    "#TEST_BATCH_SIZE = 10\n",
    "\n",
    "#train_step = int(TRAIN_DATA_SIZE/TRAIN_BATCH_SIZE)\n",
    "#test_step = int(TEST_DATA_SIZE/TEST_BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "train_gen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "train_gen.fit(x_train)\n",
    "#train_batches = train_gen.flow(x_train, y_train, batch_size=TRAIN_BATCH_SIZE, shuffle=False, seed=10)\n",
    "\n",
    "test_gen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "test_gen.fit(x_train)\n",
    "#test_batches = test_gen.flow(x_test, y_test, batch_size=TEST_BATCH_SIZE, shuffle=False, seed=10)\n",
    "\n",
    "#row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_batch_size = 0\n",
    "best_epochs = 0\n",
    "best_optimizer = ''\n",
    "best_learning_rate = 0\n",
    "best_node_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4083
    },
    "colab_type": "code",
    "id": "4Pipxp3mXlUq",
    "outputId": "59b29f67-3cd3-426a-cd9d-6194d22149ed"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed41e34f91ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moptimizer_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizers_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "  for epochs in num_epochs:\n",
    "    for optimizer_name in optimizers_name:\n",
    "      for learning_rate in learning_rates:\n",
    "        for node_size in node_sizes:\n",
    "            print(\"==================\")\n",
    "            print(\"batch size    : {}\".format(batch_size))\n",
    "            print(\"epochs        : {}\".format(epochs))\n",
    "            print(\"optimizer     : {}\".format(optimizer_name))\n",
    "            print(\"learning rate : {}\".format(learning_rate))\n",
    "            print(\"node size     : {}\".format(node_size))\n",
    "\n",
    "            if optimizer_name == 'adam':\n",
    "              optimizer = optimizers.Adam(lr=learning_rate)\n",
    "            elif optimizer_name == 'rmsprop':\n",
    "              optimizer = optimizers.RMSprop(lr=learning_rate)\n",
    "            elif optimizer_name == 'sgd':\n",
    "              optimizer = optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "            # Embedding dimensions.\n",
    "            row_hidden = node_size\n",
    "            col_hidden = node_size\n",
    "\n",
    "            \n",
    "            TRAIN_BATCH_SIZE = batch_size\n",
    "            TEST_BATCH_SIZE = (batch_size)\n",
    "            TEST_BATCH_SIZE\n",
    "            \n",
    "            train_step = int(TRAIN_DATA_SIZE/TRAIN_BATCH_SIZE)\n",
    "            test_step = int(TEST_DATA_SIZE/TEST_BATCH_SIZE)\n",
    "            \n",
    "            train_batches = train_gen.flow(x_train, y_train, batch_size=TRAIN_BATCH_SIZE, shuffle=False, seed=10)\n",
    "\n",
    "            test_batches = test_gen.flow(x_test, y_test, batch_size=TEST_BATCH_SIZE, shuffle=False, seed=10)\n",
    "\n",
    "            row, col, pixel = x_train.shape[1:]\n",
    "            \n",
    "            \n",
    "            # 4D input.\n",
    "            x = Input(shape=(row, col, pixel))\n",
    "\n",
    "            # Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "            encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "            # Encodes columns of encoded rows.\n",
    "            encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "            # Final predictions and model.\n",
    "            prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "\n",
    "            model = Model(x, prediction)\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            # Training.\n",
    "            # model.fit(x_train, y_train,\n",
    "            #           batch_size=batch_size,\n",
    "            #           epochs=epochs,\n",
    "            #           verbose=1,\n",
    "            #           validation_data=(x_test, y_test))\n",
    "            \n",
    "           # model_file_name = 'models/' + model_type + '.' + str(num_of_train_layer) + '-' + optimizer + '-' + str(learning_rate) + '-' + str(epoch) + '.{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.h5'\n",
    "\n",
    "  #model_file_name = 'models/' + model_type + '.' + str(num_of_train_layer) + '-' + optimizer + '-' + str(learning_rate) + '-' \n",
    "#+ str(epoch) + '.{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.h5'\n",
    "            model_file_name = \"models2/rnn_lstm_{}_{}_{}_{}_{}_{}_{}.h5\" . format(batch_size, epochs, optimizer_name, learning_rate, node_size,val_acc,val_loss)\n",
    "            checkpoint = ModelCheckpoint(model_file_name, monitor='val_loss', verbose=0, save_best_only= True, mode='auto')\n",
    "\n",
    "            best_model_file_name = \"models2/rnn_lstm.h5\"\n",
    "            best_checkpoint = ModelCheckpoint(best_model_file_name, monitor='val_loss', verbose=0, save_best_only= True, mode='auto')\n",
    "       \n",
    "            h = model.fit_generator(train_batches, steps_per_epoch=train_step, validation_data=test_batches, callbacks=[checkpoint, best_checkpoint], validation_steps=test_step, epochs=epochs, verbose=0)\n",
    "            #h = model.fit_generator(train_batches, validation_data=test_batches, callbacks=[checkpoint, best_checkpoint], verbose=1)\n",
    "  \n",
    "            plot_model(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Bm1fLzDLXlUx"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(best_model)\n",
    "print(\"Best Stats\")\n",
    "print(\"==========\")\n",
    "print(\"accuracy      : {}\".format(best_accuracy))\n",
    "print(\"batch size    : {}\".format(best_batch_size))\n",
    "print(\"epochs        : {}\".format(best_epochs))\n",
    "print(\"optimizer     : {}\".format(best_optimizer))\n",
    "print(\"learning rate : {}\".format(best_learning_rate))\n",
    "print(\"node size     : {}\".format(best_node_size))\n",
    "\n",
    "filepath = \"models/rnn_lstm_best.h5\"\n",
    "files.download(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "RNN_LSTM_Keras_working.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
